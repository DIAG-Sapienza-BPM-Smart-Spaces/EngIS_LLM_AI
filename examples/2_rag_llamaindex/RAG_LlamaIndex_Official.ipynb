{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## aggiungi installazione di librerie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAMA INDEX RAG PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import, configuration and file loading can be used for all three different framework (classic RAG, RAG with KG and RAG with ontologies). \"Query and response\" block is reported for each framework for seimplicity but it can be used for all as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai  \n",
    "import os  \n",
    "import fitz  \n",
    "import nest_asyncio\n",
    "\n",
    "from rdflib import Graph\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.core import (\n",
    "    Document, \n",
    "    VectorStoreIndex, \n",
    "    PropertyGraphIndex,\n",
    "    SimpleDirectoryReader\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the asyncio patch to allow nested event loops.\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables from a .env file.\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the OpenAI and Llama API key from the environment variables.\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify required extensions as a list (if needed)\n",
    "required_exts = [] # .png, .jpg, .pdf, .md, .txt, .csv etc...\n",
    "\n",
    "# Load documents from a specified directory\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=\"C:/Users/filow/OneDrive/Desktop/Phd/02_Secondo anno/PAPERS/03_Paper_CAiSE_Technical_2025_[Submitted]/OFF_CODE_01_25/document\",\n",
    "    input_files=[\"C:/Users/filow/OneDrive/Desktop/Phd/02_Secondo anno/PAPERS/03_Paper_CAiSE_Technical_2025_[Submitted]/OFF_CODE_01_25/document/ont_sm.owl\"], #,\n",
    "                #\"document/document_2\"]\n",
    "    required_exts=required_exts\n",
    ")\n",
    "\n",
    "# Load and parse document from the reader\n",
    "document = reader.load_data(num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_file = str(reader.input_files[0])\n",
    "print(first_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIC RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector index from the parsed documents.\n",
    "index = VectorStoreIndex.from_documents(document)\n",
    "\n",
    "# Initialize the OpenAI language model (LLM) for generating responses.\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    strict=True,        # True default\n",
    "    temperature=0,      # from 0 to 1\n",
    "    max_tokens=2000,    # from 0 to infinity (None default)\n",
    "    top_p=0.8)          # from 0 to 1\n",
    "\n",
    "# Convert the vector index into a query engine for running queries on the indexed documents.\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode = \"compact\", \n",
    "    streaming = True,\n",
    "    similarity_top_k=2,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert you query \n",
    "user_query = input(\"Enter your query: \")\n",
    "\n",
    "# Execute a query and retrieve the response\n",
    "response = query_engine.query(user_query)\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG WITH KG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PropertyGraphIndex from the parsed documents.\n",
    "index = PropertyGraphIndex.from_documents(document)\n",
    "\n",
    "# Initialize the OpenAI language model (LLM) for generating responses.\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    strict=True,        # True default\n",
    "    temperature=0,      # from 0 to 1\n",
    "    max_tokens=2000,    # from 0 to infinity (None default)\n",
    "    top_p=0.8)          # from 0 to 1\n",
    "\n",
    "# Convert the vector index into a query engine for running queries on the indexed documents.\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode = \"compact\", \n",
    "    streaming = True,\n",
    "    similarity_top_k=2,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert you query \n",
    "user_query = input(\"Enter your query: \")\n",
    "\n",
    "# Execute a query and retrieve the response\n",
    "response = query_engine.query(user_query)\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONTOLOGY RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an RDF graph and parse the selected XML file into it.\n",
    "g = Graph()\n",
    "g.parse(str(reader.input_files[0]), format=\"xml\") # must be .owl, .json or .csv\n",
    "\n",
    "# Create a list of nodes from the RDF graph\n",
    "nodes = []\n",
    "documents_ontology = []\n",
    "\n",
    "for s, p, o in g:\n",
    "    nodes.append({\n",
    "        \"subject\": str(s),\n",
    "        \"predicate\": str(p),\n",
    "        \"object\": str(o), })\n",
    "\n",
    "# Generate documents from RDF nodes and store them in the documents_ontology list.\n",
    "for node in nodes:\n",
    "    content = f\"Subject: {node['subject']}, Predicate: {node['predicate']}, Object: {node['object']}\"\n",
    "    documents_ontology.append(Document(text=content)) \n",
    "\n",
    "# Index the documents into a PropertyGraphIndex for semantic search.\n",
    "index_ontology = PropertyGraphIndex.from_documents(documents_ontology)\n",
    "\n",
    "# Initialize the OpenAI model with specific configurations for response generation.\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    strict=True,        # True default\n",
    "    temperature=0,      # from 0 to 1\n",
    "    max_tokens=2000,    # from 0 to infinity (None default)\n",
    "    top_p=0.8),         # from 0 to 1\n",
    "\n",
    "# Set up the query engine to perform semantic search on ontology index.\n",
    "query_engine = index_ontology.as_query_engine(\n",
    "    llm=Settings.llm,\n",
    "    response_mode = \"compact\", \n",
    "    streaming = True,\n",
    "    similarity_top_k=2,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert you query \n",
    "user_query = input(\"Enter your query: \")\n",
    "\n",
    "# Execute a query and retrieve the response\n",
    "response = query_engine.query(user_query)\n",
    "print(f\"Answer: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
