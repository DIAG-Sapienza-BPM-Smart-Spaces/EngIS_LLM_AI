{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG EVALUATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary evaluation metrics from the deepeval library\n",
    "from deepeval.metrics import ContextualPrecisionMetric  \n",
    "from deepeval.metrics import ContextualRecallMetric     \n",
    "from deepeval.metrics import AnswerRelevancyMetric      \n",
    "from deepeval.metrics import FaithfulnessMetric    \n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval import evaluate\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics configuration and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the environment variable for OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-key-here\"\n",
    "\n",
    "# Initializing test inputs\n",
    "actual_output = \"\"       \n",
    "expected_output = \"\"    \n",
    "retrieval_context = [] \n",
    "\n",
    "# Initializing the ContextualPrecisionMetric \n",
    "metric_CP = ContextualPrecisionMetric(\n",
    "    threshold = 0.7,       \n",
    "    model = \"gpt-4o-mini\",       \n",
    "    include_reason = True \n",
    ")\n",
    "\n",
    "# Initializing the ContextualRecallMetric \n",
    "metric_CR = ContextualRecallMetric(\n",
    "    threshold = 0.7,        \n",
    "    model = \"gpt-4o-mini\",        \n",
    "    include_reason = True   \n",
    ")\n",
    "\n",
    "\n",
    "# Initializing the AnswerRelevancyMetric \n",
    "metric_AR = AnswerRelevancyMetric(\n",
    "    threshold = 0.7,        \n",
    "    model = \"gpt-4o-mini\",        \n",
    "    include_reason = True   \n",
    ")\n",
    "\n",
    "\n",
    "# Initializing the FaithfulnessMetric \n",
    "metric_F = FaithfulnessMetric(\n",
    "    threshold = 0.7,        \n",
    "    model = \"gpt-4o-mini\",        \n",
    "    include_reason = True   \n",
    ")\n",
    "\n",
    "\n",
    "# Creating a test case with the necessary inputs to evaluate the metric\n",
    "test_case = LLMTestCase(\n",
    "    input = \"Where is the tour eiffel?\",           \n",
    "    actual_output = actual_output, \n",
    "    expected_output = expected_output,  \n",
    "    retrieval_context = retrieval_context  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a test case to evaluate contextual precision (CP)\n",
    "test_case_CP = LLMTestCase(\n",
    "    input = \"\",           \n",
    "    actual_output = actual_output, \n",
    "    expected_output = expected_output,  \n",
    "    retrieval_context = retrieval_context  \n",
    ")\n",
    "\n",
    "# Creating a test case to evaluate contextual recall (CR)\n",
    "test_case_CR = LLMTestCase(\n",
    "    input = \"\",           \n",
    "    actual_output = actual_output, \n",
    "    expected_output = expected_output,  \n",
    "    retrieval_context = retrieval_context  \n",
    ")\n",
    "\n",
    "# Creating a test case to evaluate answer relevancy (AR)\n",
    "test_case_AR = LLMTestCase(\n",
    "    input = \"\",           \n",
    "    actual_output = actual_output,  \n",
    ")\n",
    "\n",
    "# Creating a test case to evaluate faithfulness (F)\n",
    "test_case_F = LLMTestCase(\n",
    "    input = \"\",           \n",
    "    actual_output = actual_output,  \n",
    "    retrieval_context = retrieval_context  \n",
    ")\n",
    "\n",
    "# Measuring the evaluation score for the test case\n",
    "metric_CP.measure(test_case_CP)\n",
    "metric_CR.measure(test_case_CR)  \n",
    "metric_AR.measure(test_case_AR)  \n",
    "metric_F.measure(test_case_F)  \n",
    "\n",
    "# Evaluating the test case with the metric and displaying results\n",
    "evaluate([test_case_CP], [metric_CP])  \n",
    "evaluate([test_case_CR], [metric_CR])  \n",
    "evaluate([test_case_AR], [metric_AR])  \n",
    "evaluate([test_case_F], [metric_F])  \n",
    "\n",
    "# Printing results\n",
    "print(f\"metric CP score: {metric_CP.score}\") \n",
    "print(f\"metric CP reason: {metric_CP.reason}\") \n",
    "\n",
    "# Printing the score and reasoning behind it\n",
    "print(f\"metric CR score: {metric_CR.score}\") \n",
    "print(f\"metric CR reason: {metric_CR.reason}\") \n",
    "\n",
    "# Printing the score and reasoning behind it\n",
    "print(f\"metric AR score: {metric_AR.score}\") \n",
    "print(f\"metric AR reason: {metric_AR.reason}\") \n",
    "\n",
    "# Printing the score and reasoning behind it\n",
    "print(f\"metric F score: {metric_F.score}\") \n",
    "print(f\"metric F reason: {metric_F.reason}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
